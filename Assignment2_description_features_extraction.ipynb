{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import urllib\n",
    "import googlemaps\n",
    "import json\n",
    "import time, datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "panda_data = pandas.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('records.json', 'w') as outfile:\n",
    "    json.dump(json.loads(panda_data.to_json(orient='records')), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.urlopen(fname):\n",
    "        yield eval(l)\n",
    "        \n",
    "data = np.transpose(list(parseData('records.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . display_address : Metropolitan Avenue\n",
      "2 . description : A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy These Following Apartment Features As You Rent Here? Modern Designed Bathroom w/ a Deep Spa Soaking Tub? Room to Room AC/Heat? Real Oak Hardwood Floors? Rain Forest Shower Head? SS steel Appliances w/ Chef Gas Cook Oven & LG Fridge? washer /dryer in the apt? Cable Internet Ready? Granite Counter Top Kitchen w/ lot of cabinet storage spaceIt's Just A Few blocks To L Train<br /><br />Don't miss out!<br /><br />We have several great apartments in the immediate area.<br /><br />For additional information 687-878-2229<p><a  website_redacted \n",
      "3 . price : 3000\n",
      "4 . bedrooms : 3\n",
      "5 . photos : ['https://photos.renthop.com/2/7211212_1ed4542ec81621d70d1061aa833e669c.jpg', 'https://photos.renthop.com/2/7211212_7dfc41dced69245065df83d08eed4a00.jpg', 'https://photos.renthop.com/2/7211212_c17853c4b869af6f53af08b0f5820b4c.jpg', 'https://photos.renthop.com/2/7211212_787ad8ea0c089792e7453e2121f8ac89.jpg', 'https://photos.renthop.com/2/7211212_2e88b0d293ee333c804c2f00536eee49.jpg']\n",
      "6 . bathrooms : 1.5\n",
      "7 . features : []\n",
      "8 . listing_id : 7211212\n",
      "9 . building_id : 53a5b119ba8f7b61d4e010512e0dfc85\n",
      "10 . created : 2016-06-24 07:54:24\n",
      "11 . longitude : -73.9425\n",
      "12 . manager_id : 5ba989232d0489da1b5f2c45f6688adc\n",
      "13 . latitude : 40.7145\n",
      "14 . interest_level : medium\n",
      "15 . street_address : 792 Metropolitan Avenue\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate(data[0][0]):\n",
    "    print i+1, '.', k, ':', data[0][0][k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Saransh/anaconda/envs/tensorflow_env/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text, stemmer=PorterStemmer()):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "def trainTfIdfModel(strings):\n",
    "    content = np.array([])\n",
    "    tfIdfScores = np.array([])\n",
    "    print 'Preprocessing Data'\n",
    "    for s in strings:\n",
    "        t = \" \".join(item.strip() for item in BeautifulSoup(s).find_all(text=True))\n",
    "        t = str(''.join([i if ord(i) < 128 else ' ' for i in t]))\n",
    "        replace_punctuation = string.maketrans(punctuation, ' '*len(punctuation))\n",
    "        transformed_string = t.lower().translate(replace_punctuation)\n",
    "        content = np.append(content, transformed_string)\n",
    "     \n",
    "    print 'Started Training'\n",
    "    tfs = tfidf.fit_transform(content)\n",
    "\n",
    "trainTfIdfModel([d[0]['description'] for d in data])\n",
    "\n",
    "def getTransformedtfIdfScore(review):\n",
    "    t = \" \".join(item.strip() for item in BeautifulSoup(review).find_all(text=True))\n",
    "    t = str(''.join([i if ord(i) < 128 else ' ' for i in t]))\n",
    "    replace_punctuation = string.maketrans(punctuation, ' '*len(punctuation))\n",
    "    transformed_string = t.lower().translate(replace_punctuation)\n",
    "    response = tfidf.transform([transformed_string])\n",
    "    \n",
    "#     feature_names = tfidf.get_feature_names()\n",
    "#     for col in response.nonzero()[1]:\n",
    "#         print feature_names[col], ' - ', response[0, col]\n",
    "    \n",
    "#     print response.nonzero()[1]\n",
    "    if float(response.nonzero()[1].shape[0]) == 0:\n",
    "        return 0\n",
    "    return float(np.sum([response[0, col] for col in response.nonzero()[1]])) / float(response.nonzero()[1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getARI(string):\n",
    "    string = \" \".join(item.strip() for item in BeautifulSoup(string).find_all(text=True))\n",
    "    string = str(''.join([i if ord(i) < 128 else ' ' for i in string]))\n",
    "    s_count = float(len(nltk.sent_tokenize(string)))\n",
    "    w_count = float(len(nltk.wordpunct_tokenize(string)))\n",
    "    c_count = float(len(string))\n",
    "    \n",
    "    if s_count == 0:\n",
    "        return 12\n",
    "    \n",
    "    return math.ceil((4.71*(c_count/w_count)) + (0.5*(w_count/s_count)) - 21.43)\n",
    "\n",
    "\n",
    "\n",
    "def GetFeatures(data):\n",
    "    structured_data = []\n",
    "    index = 1\n",
    "    for d in data:\n",
    "        feat = []\n",
    "        if index % 10000 == 1:\n",
    "            print 'Starting', index\n",
    "            \n",
    "        feat.append(time.mktime(datetime.datetime.strptime(d[0]['created'], \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "        feat.append(getTransformedtfIdfScore(d[0]['description']))\n",
    "        feat.append(getARI(d[0]['description']))\n",
    "        structured_data.append(feat)\n",
    "        if index % 10000 == 1:\n",
    "            print 'Done', index\n",
    "        index += 1\n",
    "    \n",
    "    return structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1\n",
      "Done 1\n",
      "Starting 10001\n",
      "Done 10001\n",
      "Starting 20001\n",
      "Done 20001\n",
      "Starting 30001\n",
      "Done 30001\n",
      "Starting 40001\n",
      "Done 40001\n"
     ]
    }
   ],
   "source": [
    "new_data = GetFeatures(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1466780064.0, 0.10791517320475999, 6.0]\n",
      "(49352, 3) (49352,)\n"
     ]
    }
   ],
   "source": [
    "print new_data[0]\n",
    "print np.array(new_data).shape, np.array(new_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1466780064.0, 0.10791517320475999, 6.0], [1465759167.0, 0, 12], [1460888801.0, 0.11633887623912888, 12.0], [1460971322.0, 0.15148662481922268, 11.0], [1461832361.0, 0.13460773829788475, 11.0], [1461065087.0, 0, 12], [1461752396.0, 0.11315999966405803, 17.0], [1460552502.0, 0.09805834665654639, 48.0], [1461144995.0, 0, 12], [1459591095.0, 0.10279767184574277, 14.0]]\n"
     ]
    }
   ],
   "source": [
    "print new_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('ut_tfidf_ari', new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "insecure string pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-06715cea9488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interest_prediction/data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdhruv_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Saransh/anaconda/envs/tensorflow_env/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Saransh/anaconda/envs/tensorflow_env/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Saransh/anaconda/envs/tensorflow_env/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"insecure string pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m                 \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: insecure string pickle"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('interest_prediction/data', 'r') as dump_file:\n",
    "    dhruv_data = pickle.load(dump_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
